{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40801339-8485-4e08-9847-447731c90298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_base_path = \"abfss://bronze@adfassignment23.dfs.core.windows.net/sales-view\"\n",
    "silver_base_path = \"abfss://silver@adfassignment23.dfs.core.windows.net/sales-view\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5150ad-ca91-4afc-b00c-f2e162913a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/sce21cs010@sairamtap.edu.in/adf_assignment/src/bronze_to_silver/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db943a16-99ac-4738-845c-f3445ad8587e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, when, to_date\n",
    "import re\n",
    "\n",
    "# Step 1: Read CSV from Bronze Layer\n",
    "df_customer = spark.read.option(\"header\", True).format(\"csv\").load(f\"{bronze_base_path}/customer/\")\n",
    "display(df_customer)\n",
    "\n",
    "# Step 2: Clean and Rename Columns\n",
    "df_customer = clean_and_snake_case_columns(df_customer)\n",
    "df_customer.printSchema()\n",
    "display(df_customer)\n",
    "\n",
    "# Step 3: Extract first_name and last_name from 'name'\n",
    "df_customer = df_customer.withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0)) \\\n",
    "                         .withColumn(\"last_name\", split(col(\"name\"), \" \").getItem(1))\n",
    "\n",
    "# Step 4: Extract domain from 'email_id'\n",
    "df_customer = df_customer.withColumn(\"domain\", split(col(\"email_id\"), \"@\").getItem(1)) \\\n",
    "                         .withColumn(\"domain\", split(col(\"domain\"), r\"\\.\").getItem(0))\n",
    "\n",
    "# Step 5: Convert gender values to M/F/O\n",
    "df_customer = df_customer.withColumn(\"gender\", when(col(\"gender\") == \"male\", \"M\")\n",
    "                                               .when(col(\"gender\") == \"female\", \"F\")\n",
    "                                               .otherwise(\"O\"))\n",
    "\n",
    "# Step 6: Split joining_date into date and time\n",
    "df_customer = df_customer.withColumn(\"joining_date_split\", split(col(\"joining_date\"), \" \")) \\\n",
    "                         .withColumn(\"date\", to_date(col(\"joining_date_split\").getItem(0), \"MM-dd-yyyy\")) \\\n",
    "                         .withColumn(\"time\", col(\"joining_date_split\").getItem(1)) \\\n",
    "                         .drop(\"joining_date_split\")\n",
    "\n",
    "# Step 7: Create expenditure_status based on 'spent'\n",
    "df_customer = df_customer.withColumn(\"spent_numeric\", col(\"spent\").cast(\"double\")) \\\n",
    "                         .withColumn(\"expenditure_status\", \n",
    "                                     when(col(\"spent_numeric\") < 200, \"MINIMUM\")\n",
    "                                     .otherwise(\"MAXIMUM\")) \\\n",
    "                         .drop(\"spent_numeric\")\n",
    "\n",
    "display(df_customer)\n",
    "\n",
    "# Step 8: Write to Silver Layer as Delta Table\n",
    "df_customer.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/customer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac023c82-b150-4a04-8dd6-19dcbbf98874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Read CSV from Bronze layer\n",
    "df_product = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/product\")\n",
    "display(df_product)\n",
    "\n",
    "# Clean and convert column names to snake_case\n",
    "df_product = clean_and_snake_case_columns(df_product)\n",
    "\n",
    "# Add sub_category column based on category_id\n",
    "df_product = df_product.withColumn(\"sub_category\", \n",
    "    when(col(\"category_id\") == 1, \"phone\")\n",
    "   .when(col(\"category_id\") == 2, \"laptop\")\n",
    "   .when(col(\"category_id\") == 3, \"playstation\")\n",
    "   .when(col(\"category_id\") == 4, \"e-device\")\n",
    ")\n",
    "\n",
    "display(df_product)\n",
    "\n",
    "# Write to Silver layer as Delta table\n",
    "df_product.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b329288f-759d-4219-bc70-62442f1cdf7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, regexp_extract, to_date, when\n",
    "\n",
    "# Step 1: Reading CSV from Bronze\n",
    "df_store = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/store\")\n",
    "df_store.display()\n",
    "\n",
    "# Step 2: Cleaning and renaming columns to snake_case\n",
    "df_store = clean_and_snake_case_columns(df_store)\n",
    "\n",
    "# Step 3: Extracting store_category from email domain\n",
    "df_store = df_store.withColumn(\n",
    "    \"store_category\",\n",
    "    split(split(col(\"email_address\"), \"@\").getItem(1), r\"\\.\").getItem(0)\n",
    ")\n",
    "\n",
    "# Step 4: Defining regex pattern for 'dd-MM-yyyy'\n",
    "date_pattern = r\"^\\d{2}-\\d{2}-\\d{4}$\"\n",
    "\n",
    "# Step 5: Converting `created_at` only if it matches the date pattern\n",
    "df_store = df_store.withColumn(\n",
    "    \"created_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"created_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"created_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Step 6: Converting `updated_at` only if it matches the date pattern\n",
    "df_store = df_store.withColumn(\n",
    "    \"updated_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"updated_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"updated_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_store.display()\n",
    "\n",
    "# Step 7: Write to Silver Layer as Delta table\n",
    "df_store.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{silver_base_path}/store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b260b0-626e-42ac-bf4d-cba5344ce4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Step 1: Read CSV from Bronze Layer\n",
    "df_sales = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/sales\")\n",
    "display(df_sales) \n",
    "\n",
    "# Step 2: Clean and convert column names to snake_case\n",
    "df_sales = clean_and_snake_case_columns(df_sales)\n",
    "display(df_sales)\n",
    "\n",
    "# Step 3: Format timestamp columns\n",
    "df_sales = df_sales.withColumn(\n",
    "    \"order_date\", to_timestamp(col(\"order_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ").withColumn(\n",
    "    \"ship_date\", to_timestamp(col(\"ship_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ")\n",
    "\n",
    "# Step 4: Write to Silver Layer in Delta format\n",
    "df_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"{silver_base_path}/customer_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3d2e92-b318-4f21-9351-cf240a223cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ssh-keygen -t rsa -b 4096 -C \"shreyaagarwal.g@diggibyte.com\" -f ~/.ssh/id_rsa -N \"\"\n",
    "cat ~/.ssh/id_rsa.pub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e52f72-7ff9-4e73-8933-1127102a09bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip\n",
    "!pip install databricks-cli\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c200f072-9449-4796-9222-0d78487d3259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52169b66-857a-47b0-9699-882d3da23977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!databricks configure --token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386eee9c-33dd-457e-a8aa-b2e8d998c41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!databricks secrets create-scope --scope git-ssh\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6888926777402814,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "drivers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
